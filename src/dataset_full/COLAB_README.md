# ğŸš€ Google Colab Training Guide

## ğŸ“‹ Quick Start (Recommended Method)

### âœ… **Using GitHub Repository** (EASIEST!)

**Colab Setup**:
- Runtime type: **Python 3**
- Hardware accelerator: **GPU (T4, V100, or A100)** âš¡
- Runtime shape: **High-RAM** (52GB)

---

## ğŸ”§ Setup Instructions

### 1. Clone Repository

```python
# Clone the project from GitHub
!git clone https://github.com/lamdx4/iot-prj iot-prj
```

**Note**: Repository already includes:
- âœ… All training scripts
- âœ… Pre-processed batch files (in Data/Dataset/merged_batches/)
- âœ… Complete pipeline ready to run

---

### 2. Install Dependencies

```python
!pip install -q xgboost scikit-learn imbalanced-learn pandas numpy joblib psutil matplotlib seaborn
```

---

### 3. Run Training

```python
# Train the two-stage hierarchical model
!python ./iot-prj/src/dataset_full/scripts/03_train_colab_highmem.py
```

**Training Configuration**:
- Train: batch_01 + batch_04 (~20M records)
- Test: batch_02 sampled (300K records)
- RAM: ~33 GB peak usage
- GPU: Uses `tree_method="gpu_hist"` + `gpu_predictor` automatically
- Time: ~15-20 minutes (with GPU)

**Expected Output**:
```
================================================================================
TRAIN TWO-STAGE MODEL - COLAB PRO+ HIGH-RAM
================================================================================
ğŸš€ GPU detected: Tesla T4, 15360 MiB
   GPU Memory: 15.0 GB

ğŸ“‚ Loading batch_01... âœ… 10,000,000 records
ğŸ“‚ Loading batch_04... âœ… 10,000,000 records
âœ… Training data: 20,000,000 records

ğŸš€ Training Stage 1... (Attack vs Normal)
   âœ… Best iteration: 127, Score: 0.0023

ğŸ“Š Stage 1 Performance:
   Accuracy:  0.9987
   Precision: 0.9989
   Recall:    0.9996
   F1-Score:  0.9992

ğŸš€ Training Stage 2... (DDoS, DoS, Reconnaissance)
   âœ… Best iteration: 156, Score: 0.0156

ğŸ“Š Overall Accuracy: 0.9245 (92.45%)

âœ… Models saved to: models/full_dataset/
```

---

### 4. Evaluate Models

```python
# Run evaluation on test set
!python ./iot-prj/src/dataset_full/scripts/04_evaluate_model.py
```

---

### 5. Generate Visualizations

```python
# Create charts and plots
!python ./iot-prj/src/dataset_full/scripts/05_visualize_results.py
```

This will generate 7 visualization files in `models/full_dataset/visualizations/`:
1. `01_confusion_matrix.png`
2. `02_confusion_matrix_normalized.png`
3. `03_per_category_metrics.png`
4. `04_stage_comparison.png`
5. `05_class_distribution.png`
6. `06_accuracy_vs_support.png`
7. `00_summary_dashboard.png`

---

### 6. Download Results

```python
# Zip models and evaluation results
!zip -r /content/models.zip /content/iot-prj/models/
!zip -r /content/evaluation.zip /content/iot-prj/models/full_dataset/visualizations/

# Download
from google.colab import files
files.download('/content/models.zip')
files.download('/content/evaluation.zip')
```

---

## ğŸš€ GPU Optimization

The training script **automatically detects and uses GPU** if available!

### GPU Detection Output:
```
ğŸš€ GPU detected: Tesla T4, 15360 MiB
   GPU Memory: 15.0 GB
   Using: gpu_hist (GPU âš¡)
```

### Check GPU Availability:
```python
# Verify GPU is enabled
!nvidia-smi
```

### XGBoost GPU Parameters (Auto-configured):
```python
XGBClassifier(
    tree_method="gpu_hist",      # Use GPU for tree construction
    predictor="gpu_predictor",   # Use GPU for prediction
    n_jobs=1,                     # GPU handles parallelism internally
    max_bin=512,                  # More bins for GPU
    ...
)
```

### Speed Comparison:
| GPU Type | Training Time (20M records) |
|----------|----------------------------|
| **CPU only** | ~2-3 hours |
| **T4** | ~15-20 minutes |
| **V100** | ~10-15 minutes |
| **A100** | ~5-10 minutes |

---

## ğŸ’¾ Memory Usage

The script uses **batch_01 + batch_04** (~20M records) to fit in 52GB High-RAM:

```
Loading batch_01... âœ… 10M records â†’ RAM: ~5 GB
Loading batch_04... âœ… 10M records â†’ RAM: ~8 GB
After preprocessing... â†’ RAM: ~15-20 GB
During training... â†’ RAM: ~25-33 GB (peak)
```

Memory is automatically managed with garbage collection after each major step.

---

## ğŸ¯ Recommendations

### For Best Performance:
1. âœ… Use **Colab Pro+ High-RAM** (52 GB)
2. âœ… Enable **GPU** (T4 or better)
3. âœ… Clone repo directly (no manual uploads)
4. âœ… Let training run ~15-20 minutes
5. âœ… Expected accuracy: **90-95%**

---

## ğŸ› Troubleshooting

### "Repository not found" or Clone fails
```python
# Make sure the repo is public or use authentication
!git clone https://github.com/lamdx4/iot-prj iot-prj
```

### "CUDA out of memory"
- Colab GPU has limited VRAM (15GB for T4)
- **Solution**: Script is already optimized for T4 GPU
- If still occurs, restart runtime and try again

### "RAM/Disk quota exceeded"
- Free Colab: ~12GB RAM âŒ Not enough
- Colab Pro: ~25GB RAM âš ï¸ Might work
- **Colab Pro+ High-RAM: ~52GB** âœ… Recommended
- **Solution**: Upgrade to High-RAM runtime

### "Training very slow" (Taking hours)
Check if GPU is enabled:
```python
!nvidia-smi
```
If no output, GPU is not enabled.
- **Solution**: Runtime â†’ Change runtime type â†’ GPU

### Import errors
```python
# Reinstall packages
!pip install --upgrade xgboost scikit-learn imbalanced-learn matplotlib seaborn
```

### "File not found" errors
Make sure you cloned the repo correctly:
```python
# Check repo structure
!ls -lh iot-prj/
!ls -lh iot-prj/src/dataset_full/scripts/
!ls -lh iot-prj/Data/Dataset/merged_batches/
```

---

## ğŸ“ File Structure After Clone

```
/content/
â””â”€â”€ iot-prj/
    â”œâ”€â”€ Data/
    â”‚   â””â”€â”€ Dataset/
    â”‚       â””â”€â”€ merged_batches/        â† Pre-processed batch files
    â”‚           â”œâ”€â”€ batch_01.csv  (~2.6 GB)
    â”‚           â”œâ”€â”€ batch_02.csv  (~2.6 GB)
    â”‚           â”œâ”€â”€ batch_04.csv  (~2.6 GB)
    â”‚           â””â”€â”€ batch_05.csv  (~2.6 GB)
    â”‚
    â”œâ”€â”€ src/
    â”‚   â””â”€â”€ dataset_full/
    â”‚       â””â”€â”€ scripts/
    â”‚           â”œâ”€â”€ 03_train_colab_highmem.py  â† Training script
    â”‚           â”œâ”€â”€ 04_evaluate_model.py       â† Evaluation
    â”‚           â””â”€â”€ 05_visualize_results.py    â† Visualization
    â”‚
    â””â”€â”€ models/                        â† Created during training
        â””â”€â”€ full_dataset/
            â”œâ”€â”€ stage1_*.pkl
            â”œâ”€â”€ stage2_*.pkl
            â”œâ”€â”€ encoders_*.pkl
            â”œâ”€â”€ mapping_*.pkl
            â”œâ”€â”€ features_*.pkl
            â”œâ”€â”€ training_metrics_*.json
            â””â”€â”€ visualizations/        â† Created by step 5
                â”œâ”€â”€ 00_summary_dashboard.png
                â”œâ”€â”€ 01_confusion_matrix.png
                â””â”€â”€ ...
```

---

## â±ï¸ Estimated Time & Resources

| Step | Time | RAM Peak | GPU Usage |
|------|------|----------|-----------|
| 1. Clone repo | ~2 min | 1 GB | - |
| 2. Install deps | ~1 min | 1 GB | - |
| 3. Train models | ~15-20 min | 33 GB | High |
| 4. Evaluate | ~3 min | 10 GB | Medium |
| 5. Visualize | ~2 min | 5 GB | - |
| 6. Download | ~2 min | - | - |
| **Total** | **~25-30 min** | **33 GB** | **GPU required** |

---

## ğŸ“ Tips

1. **Keep tab active**: Colab may disconnect if idle during training
2. **Monitor progress**: Script prints detailed progress logs
3. **GPU utilization**: Check with `!nvidia-smi` in another cell
4. **Save checkpoints**: Models are saved automatically with timestamps
5. **Download immediately**: Runtime resets after 12 hours
6. **Batch files included**: No need to upload large CSV files manually!

---

## ğŸ“ Quick Reference

### Check System Info:
```python
# GPU
!nvidia-smi

# RAM
!free -h

# Disk space
!df -h

# Python packages
!pip list | grep -E "xgboost|scikit|imbalanced"
```

### Verify Files:
```python
# Check batch files exist
!ls -lh iot-prj/Data/Dataset/merged_batches/*.csv

# Check scripts
!ls -lh iot-prj/src/dataset_full/scripts/*.py
```

---

## ğŸš€ All Commands in One Cell

```python
# Complete pipeline - just run this cell!

# 1. Clone repo
!git clone https://github.com/lamdx4/iot-prj iot-prj

# 2. Install dependencies
!pip install -q xgboost scikit-learn imbalanced-learn pandas numpy joblib psutil matplotlib seaborn

# 3. Train
!python ./iot-prj/src/dataset_full/scripts/03_train_colab_highmem.py

# 4. Evaluate
!python ./iot-prj/src/dataset_full/scripts/04_evaluate_model.py

# 5. Visualize
!python ./iot-prj/src/dataset_full/scripts/05_visualize_results.py

# 6. Download results
!zip -r /content/models.zip /content/iot-prj/models/
!zip -r /content/evaluation.zip /content/iot-prj/models/full_dataset/visualizations/

from google.colab import files
files.download('/content/models.zip')
files.download('/content/evaluation.zip')

print("\nâœ… ALL DONE! Models and visualizations downloaded.")
```

---

**Good luck with training! ğŸš€**
