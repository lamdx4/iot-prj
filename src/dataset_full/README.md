# Full Dataset Training Pipeline

Train Two-Stage Hierarchical Model trÃªn toÃ n bá»™ Bot-IoT Dataset (74 files, ~16GB)

## ğŸ“ Structure

```
dataset_full/
â”œâ”€â”€ scripts/                          â† Training pipeline scripts
â”‚   â”œâ”€â”€ 01_merge_files.py            â† Gá»™p 10 files â†’ 1 batch
â”‚   â”œâ”€â”€ 02_analyze_batches.py        â† PhÃ¢n tÃ­ch â†’ JSON stats
â”‚   â”œâ”€â”€ 03_train_hierarchical.py     â† Train models
â”‚   â””â”€â”€ 04_test_model.py             â† Test models
â”‚
â”œâ”€â”€ stats/                            â† Statistics files
â”‚   â”œâ”€â”€ batch_statistics.json        â† Detailed stats
â”‚   â””â”€â”€ batch_summary.txt            â† Human-readable summary
â”‚
â””â”€â”€ README.md                         â† This file
```

## ğŸš€ Quick Start

### Step 1: Gá»™p Files (10 files â†’ 1 batch)

```bash
cd /home/lamdx4/Projects/IOT\ prj/src/dataset_full/scripts
python 01_merge_files.py
```

**Output:**
- `Data/Dataset/merged_batches/batch_01.csv` (files 1-10)
- `Data/Dataset/merged_batches/batch_02.csv` (files 11-20)
- ...
- `Data/Dataset/merged_batches/batch_08.csv` (files 71-74)

**Time:** ~10-15 phÃºt

---

### Step 2: PhÃ¢n tÃ­ch Batches â†’ JSON

```bash
python 02_analyze_batches.py
```

**Output:**
- `stats/batch_statistics.json` - Chi tiáº¿t tá»«ng batch
- `stats/batch_summary.txt` - TÃ³m táº¯t dá»… Ä‘á»c

**Statistics include:**
- Number of records per batch
- Class distribution (Normal, DDoS, DoS, Reconnaissance)
- Missing values
- Time range (stime, ltime)
- Protocol distribution
- Training recommendations

**Time:** ~5-10 phÃºt

---

### Step 3: Train Models

```bash
python 03_train_hierarchical.py
```

**Process:**
1. Äá»c JSON statistics
2. Chá»n best batches (prioritize batches with Normal samples)
3. Load vÃ  merge selected batches
4. Train Stage 1 (Binary: Attack vs Normal)
5. Train Stage 2 (Multi-class: DDoS, DoS, Recon)
6. Evaluate on test set (5% dataset)
7. Save models + metrics

**Output:**
- `models/full_dataset/stage1_binary_TIMESTAMP.pkl`
- `models/full_dataset/stage2_multiclass_TIMESTAMP.pkl`
- `models/full_dataset/label_encoder_TIMESTAMP.pkl`
- `models/full_dataset/attack_mapping_TIMESTAMP.pkl`
- `models/full_dataset/feature_columns_TIMESTAMP.pkl`
- `models/full_dataset/metrics_TIMESTAMP.json`

**Time:** ~10-20 phÃºt (tÃ¹y sá»‘ batches)

---

### Step 4: Test Models

```bash
python 04_test_model.py
```

**Tests:**
- Load latest trained models
- Predict on test set
- Detailed evaluation per class
- Error analysis
- Sample predictions with confidence

**Output:**
- `models/full_dataset/test_results_TIMESTAMP.json`
- Console output with detailed metrics

**Time:** ~2-3 phÃºt

---

## ğŸ“Š Pipeline Flow

```
74 Raw Files (16GB)
    â†“
[01_merge_files.py]
    â†“
8 Batch Files (~2GB each)
    â†“
[02_analyze_batches.py]
    â†“
batch_statistics.json
    â†“
[03_train_hierarchical.py]
    â†“
2 Models (Stage 1 + Stage 2)
    â†“
[04_test_model.py]
    â†“
Test Results + Metrics
```

---

## ğŸ¯ Key Features

### 1. Smart Batch Selection
- Äá»c JSON stats Ä‘á»ƒ chá»n batches
- Prioritize batches cÃ³ nhiá»u Normal samples
- Avoid training trÃªn toÃ n bá»™ dataset (memory efficient)

### 2. Imbalance Handling
- SMOTE cho Stage 1 (Normal samples)
- SMOTE cho Stage 2 (Reconnaissance minority)
- XGBoost scale_pos_weight

### 3. Comprehensive Metrics
- Per-stage metrics (Stage 1, Stage 2)
- Overall pipeline accuracy
- Confusion matrix
- Per-class accuracy
- Error analysis

### 4. Production-Ready
- Models saved vá»›i timestamp
- JSON metrics cho reproducibility
- Test script Ä‘á»ƒ verify model quality

---

## ğŸ“ˆ Expected Performance

### Dataset Stats (Full 74 files):
- Total records: ~40-50 million
- Normal samples: ~10,000-20,000 (0.02-0.05%)
- Attack samples: 99.95-99.98%
- Imbalance ratio: ~2000-5000:1

### After Merging (8 batches):
- Batch size: ~5-6 million records each
- Selected for training: Top 3-5 batches (by Normal count)
- Training size: ~15-30 million records

### Model Performance:
- Stage 1 (Binary): 99.5-99.9% accuracy
- Stage 2 (Multi-class): 98-99% accuracy
- Overall Pipeline: 98-99.5% accuracy

---

## âš™ï¸ Configuration

### Merge Settings (01_merge_files.py):
```python
BATCH_SIZE = 10  # Files per batch
```

### Training Settings (03_train_hierarchical.py):
```python
NUM_BATCHES_TO_USE = 5  # Number of batches to train on
```

Adjust based on:
- Available RAM (má»—i batch ~2GB)
- Training time requirements
- Performance needs

---

## ğŸ’¡ Tips

### Memory Management:
- **8GB RAM**: Use 2-3 batches
- **16GB RAM**: Use 4-5 batches
- **32GB+ RAM**: Use all 8 batches

### Training Speed:
- **Quick test**: 1-2 batches (~5 phÃºt)
- **Good performance**: 3-5 batches (~15 phÃºt)
- **Best performance**: 6-8 batches (~30 phÃºt)

### Batch Selection Strategy:
- Scripts tá»± Ä‘á»™ng chá»n batches vá»›i nhiá»u Normal nháº¥t
- Normal samples critical cho evaluation
- Attack distribution tÆ°Æ¡ng tá»± nhau giá»¯a batches

---

## ğŸ” Troubleshooting

### Out of Memory:
```python
# Trong 03_train_hierarchical.py
NUM_BATCHES_TO_USE = 2  # Giáº£m xuá»‘ng 2
```

### Training Too Slow:
```python
# Trong XGBClassifier
n_estimators=100  # Giáº£m tá»« 200 â†’ 100
```

### Need More Normal Samples:
â†’ Scripts Ä‘Ã£ tá»± Ä‘á»™ng chá»n batches vá»›i most Normal
â†’ Check `stats/batch_summary.txt` Ä‘á»ƒ xem distribution

---

## ğŸ“‚ Output Files

### Merged Batches:
```
Data/Dataset/merged_batches/
â”œâ”€â”€ batch_01.csv  (~2GB, files 1-10)
â”œâ”€â”€ batch_02.csv  (~2GB, files 11-20)
...
â””â”€â”€ batch_08.csv  (~0.8GB, files 71-74)
```

### Statistics:
```
src/dataset_full/stats/
â”œâ”€â”€ batch_statistics.json   (detailed stats)
â””â”€â”€ batch_summary.txt        (human-readable)
```

### Models:
```
models/full_dataset/
â”œâ”€â”€ stage1_binary_TIMESTAMP.pkl
â”œâ”€â”€ stage2_multiclass_TIMESTAMP.pkl
â”œâ”€â”€ label_encoder_TIMESTAMP.pkl
â”œâ”€â”€ attack_mapping_TIMESTAMP.pkl
â”œâ”€â”€ feature_columns_TIMESTAMP.pkl
â”œâ”€â”€ metrics_TIMESTAMP.json
â””â”€â”€ test_results_TIMESTAMP.json
```

---

## ğŸ“ Cho Äá» TÃ i

### BÃ¡o cÃ¡o nÃªn include:

1. **Dataset Description:**
   - Show batch statistics
   - Highlight extreme imbalance
   - Explain merge strategy

2. **Methodology:**
   - Two-stage hierarchical approach
   - Smart batch selection
   - Imbalance handling (SMOTE)

3. **Results:**
   - Per-stage performance
   - Overall accuracy
   - Confusion matrix
   - Error analysis

4. **Comparison:**
   - 5% dataset vs Full dataset
   - Show improvement in Normal detection
   - More reliable evaluation

---

## âœ… Advantages vs 5% Dataset

| Aspect | 5% Dataset | Full Dataset (Merged) |
|--------|------------|----------------------|
| **Normal samples** | 4 | 10,000-20,000 |
| **Evaluation reliability** | âŒ Poor | âœ… Good |
| **Training time** | 5 min | 15-30 min |
| **Memory usage** | 2GB | 8-16GB |
| **Accuracy** | 99.6% | 99.5-99.9% |
| **Production-ready** | âš ï¸ Limited | âœ… Yes |

---

**Ready to train! ğŸš€**


